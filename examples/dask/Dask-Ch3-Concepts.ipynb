{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f06651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from typing import *\n",
    "# Dask multithreading is only suited for mostly non-Python code (like pandas, numpy, etc.)\n",
    "#tag::threads[]\n",
    "dask.config.set(scheduler='threads')\n",
    "#end::threads[]\n",
    "#tag::process[]\n",
    "dask.config.set(scheduler='processes')\n",
    "#end::process[]\n",
    "#tag::dask_use_forkserver[]\n",
    "dask.config.set({\"multiprocessing.context\": \"forkserver\", \"scheduler\": \"processes\"})\n",
    "#end::dask_use_forkserver[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e93fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::make_dask_k8s_client[]\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask_kubernetes import KubeCluster, make_pod_spec\n",
    "worker_template = make_pod_spec(image='holdenk/dask:latest',\n",
    "                         memory_limit='8G', memory_request='8G',\n",
    "                         cpu_limit=1, cpu_request=1, extra_container_config={ \"imagePullPolicy\": \"Always\" })\n",
    "scheduler_template = make_pod_spec(image='holdenk/dask:latest',\n",
    "                         memory_limit='4G', memory_request='4G',\n",
    "                         cpu_limit=1, cpu_request=1, extra_container_config={ \"imagePullPolicy\": \"Always\" })\n",
    "cluster = KubeCluster(pod_template = worker_template, scheduler_pod_template = scheduler_template)\n",
    "cluster.adapt(minimum=1)    # or create and destroy workers dynamically based on workload\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "#end::make_dask_k8s_client[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6288a8-0be0-4ecd-895b-db77305ad090",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765282f3-a2da-4132-8690-c62145dd8367",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::fib_task_hello_world[]\n",
    "def dask_fib(x):\n",
    "    if x < 2:\n",
    "        return x\n",
    "    a = dask.delayed(dask_fib(x-1))\n",
    "    b = dask.delayed(dask_fib(x-2))\n",
    "    c, d = dask.compute(a, b) # Compute in parallel\n",
    "    return c + d\n",
    "\n",
    "def seq_fib(x):\n",
    "    if x < 2:\n",
    "        return x\n",
    "    return seq_fib(x-1) + seq_fib(x-2)\n",
    "\n",
    "import functools\n",
    "@functools.lru_cache\n",
    "def fib(x):\n",
    "    if x < 2:\n",
    "        return x\n",
    "    return fib(x-1) + fib(x-2)\n",
    "\n",
    "import timeit\n",
    "seq_time = timeit.timeit(lambda: seq_fib(14), number=1)\n",
    "dask_time = timeit.timeit(lambda: dask_fib(14), number=1)\n",
    "local_memoized_time = timeit.timeit(lambda: fib(14), number=1)\n",
    "print(\"In sequence {}, in parallel {}, local memoized {}\".format(seq_time, dask_time, local_memoized_time))\n",
    "#end::fib_task_hello_world[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e8ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::fail_to_ser[]\n",
    "class ConnectionClass:\n",
    "    def __init__(self, host, port):\n",
    "        import socket\n",
    "        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.socket.connect((host, port))\n",
    "\n",
    "@dask.delayed\n",
    "def bad_fun(x):\n",
    "    return ConnectionClass(\"www.scalingpythonml.com\", 80)\n",
    "\n",
    "# Fails to serialize\n",
    "if False:\n",
    "    dask.compute(bad_fun(1))\n",
    "#end::fail_to_ser[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeab452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::custom_serializer_not_own_class[]\n",
    "\n",
    "class SerConnectionClass:\n",
    "    def __init__(self, conn):\n",
    "        import socket\n",
    "        self.conn = conn\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state_dict = {\"host\": self.conn.socket.getpeername()[0], \"port\": self.conn.socket.getpeername()[1]}\n",
    "        return state_dict\n",
    "\n",
    "    def __setsate__(self, state):\n",
    "        self.conn = ConnectionClass(state[\"host\"], state[\"port\"])\n",
    "#end::custom_serializer_not_own_class[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a111b8-e787-4716-9774-a462bb366727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can sort of serialize the connection\n",
    "@dask.delayed\n",
    "def ok_fun(x):\n",
    "    return SerConnectionClass(ConnectionClass(\"www.scalingpythonml.com\", 80))\n",
    "\n",
    "dask.compute(ok_fun(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b4f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://github.com/dask/distributed/issues/5561\n",
    "@dask.delayed\n",
    "def bad_fun(x):\n",
    "    return ConnectionClass(\"www.scalingpythonml.com\", 80)\n",
    "\n",
    "from distributed.protocol import dask_serialize, dask_deserialize\n",
    "\n",
    "@dask_serialize.register(ConnectionClass)\n",
    "def serialize(bad: ConnectionClass) -> Tuple[Dict, List[bytes]]:\n",
    "    import cloudpickle\n",
    "    header = {}\n",
    "    frames = [cloudpickle.dumps({\"host\": bad.socket.getpeername()[0], \"port\": bad.socket.getpeername()[1]})]\n",
    "    return header, frames\n",
    "\n",
    "@dask_deserialize.register(ConnectionClass)\n",
    "def deserialize(bad: Dict, frames: List[bytes]) -> ConnectionClass:\n",
    "    import cloudpickle\n",
    "    info = cloudpickle.loads(frames[0])\n",
    "    return ConnectionClass(info[\"host\"], info[\"port\"])\n",
    "\n",
    "# note: this does not work because dask_serialize didn't make it to the worker :/\n",
    "# dask.compute(bad_fun(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef097dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::serialize_class_with_numpy[]\n",
    "class NumpyInfo:\n",
    "    def __init__(self, name: str, features: npt.ArrayLike):\n",
    "        self.name = name\n",
    "        self.features = features\n",
    "        \n",
    "i = NumpyInfo(\"boo\", np.array(0))\n",
    "numpybits = [i]\n",
    "\n",
    "# Surprisingly this works, despite the implication that we would need to call register_generic\n",
    "from distributed.protocol import register_generic\n",
    "register_generic(NumpyInfo)\n",
    "\n",
    "dask.compute(ok_fun(1))\n",
    "#end::serialize_class_with_numpy[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e60260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(ok_fun(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fabbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_fun(1).visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c71ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_fun(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From ch2 for visualize\n",
    "@dask.delayed\n",
    "def crawl(url, depth=0, maxdepth=1, maxlinks=4):\n",
    "    links = []\n",
    "    link_futures = []\n",
    "    try:\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        f = requests.get(url)\n",
    "        links += [(url, f.text)]\n",
    "        if (depth > maxdepth):\n",
    "            return links # base case\n",
    "        soup = BeautifulSoup(f.text, 'html.parser')\n",
    "        c = 0\n",
    "        for link in soup.find_all('a'):\n",
    "            if \"href\" in link:\n",
    "                c = c + 1\n",
    "                link_futures += crawl(link[\"href\"], depth=(depth+1), maxdepth=maxdepth)\n",
    "                # Don't branch too much were still in local mode and the web is big\n",
    "                if c > maxlinks:\n",
    "                    break\n",
    "        for r in dask.compute(link_futures):\n",
    "            links += r\n",
    "        return links\n",
    "    except requests.exceptions.InvalidSchema:\n",
    "        return [] # Skip non-web links\n",
    "import dask.bag as db\n",
    "githubs = [\"https://github.com/scalingpythonml/scalingpythonml\", \"https://github.com/dask/distributed\"]\n",
    "initial_bag = db.from_delayed(map(crawl, githubs))\n",
    "words_bag = initial_bag.map(lambda url_contents: url_contents[1].split(\" \")).flatten()\n",
    "#tag::visualize[]\n",
    "dask.visualize(words_bag.frequencies())\n",
    "#end::visualize[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af10ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(words_bag.frequencies(), filename=\"wc.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f2251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c75a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dask.array as da\n",
    "#tag::make_chunked_array[]\n",
    "distributed_array = da.from_array(list(range(0, 10000)), chunks=10)\n",
    "#end::make_chunked_array[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6647fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From ch2 so we can continue the WC example\n",
    "@dask.delayed\n",
    "def crawl(url, depth=0, maxdepth=1, maxlinks=4):\n",
    "    links = []\n",
    "    link_futures = []\n",
    "    try:\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        f = requests.get(url)\n",
    "        links += [(url, f.text)]\n",
    "        if (depth > maxdepth):\n",
    "            return links # base case\n",
    "        soup = BeautifulSoup(f.text, 'html.parser')\n",
    "        c = 0\n",
    "        for link in soup.find_all('a'):\n",
    "            if \"href\" in link:\n",
    "                c = c + 1\n",
    "                link_futures += crawl(link[\"href\"], depth=(depth+1), maxdepth=maxdepth)\n",
    "                # Don't branch too much were still in local mode and the web is big\n",
    "                if c > maxlinks:\n",
    "                    break\n",
    "        for r in dask.compute(link_futures):\n",
    "            links += r\n",
    "        return links\n",
    "    except requests.exceptions.InvalidSchema:\n",
    "        return [] # Skip non-web links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632b28d2-1cc5-4b9c-891c-c8377a4efe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "githubs = [\"https://github.com/scalingpythonml/scalingpythonml\", \"https://github.com/dask/distributed\"]\n",
    "some_bag = db.from_delayed(map(crawl, githubs))\n",
    "#tag::repartition_bag[]\n",
    "some_bag.repartition(npartitions=10)\n",
    "#end::repartition_bag[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40909d5a-0b91-4730-96f1-b213a5beea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_bag.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47906611-7eb6-4e60-9a16-659d5e2ab0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_array.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc32e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.from_dask_array(distributed_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08805e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958568d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::manual_persist[]\n",
    "df.persist\n",
    "# You do a bunch of things on DF\n",
    "\n",
    "# I'm done!\n",
    "from distributed.client import futures_of\n",
    "list(map(lambda x: x.release(), futures_of(df)))\n",
    "#end::manual_persist[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f1d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9f63d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
