{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f06651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "# Dask multithreading is only suited for mostly non-Python code (like pandas, numpy, etc.)\n",
    "#tag::threads[]\n",
    "dask.config.set(scheduler='threads')\n",
    "#end::threads[]\n",
    "#tag::process[]\n",
    "dask.config.set(scheduler='processes')\n",
    "#end::process[]\n",
    "#tag::dask_use_forkserver[]\n",
    "dask.config.set({\"multiprocessing.context\": \"forkserver\", \"scheduler\": \"processes\"})\n",
    "#end::dask_use_forkserver[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6912213a-c8a0-4ace-8147-9bcb52219820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from typing import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c63013-1ed8-4d2d-b1a0-9dec49e1672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e93fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::make_dask_k8s_client[]\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask_kubernetes import KubeCluster, make_pod_spec\n",
    "# Use load balancer to make it externally available, for purely internal\n",
    "# the default of \"ClusterIP\" is better.\n",
    "dask.config.set({\"kubernetes.scheduler-service-type\": \"LoadBalancer\"})\n",
    "worker_template = make_pod_spec(image='holdenk/dask:latest',\n",
    "                         memory_limit='8G', memory_request='8G',\n",
    "                         cpu_limit=1, cpu_request=1)\n",
    "scheduler_template = make_pod_spec(image='holdenk/dask:latest',\n",
    "                         memory_limit='4G', memory_request='4G',\n",
    "                         cpu_limit=1, cpu_request=1)\n",
    "cluster = KubeCluster(pod_template = worker_template, scheduler_pod_template = scheduler_template)\n",
    "cluster.adapt()    # or create and destroy workers dynamically based on workload\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "#end::make_dask_k8s_client[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d097c-596c-435b-914e-0623994c8044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad78e3f1-7e65-494a-a561-57596e19652e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::fib_task_hello_world[]\n",
    "def dask_fib(x):\n",
    "    if x < 2:\n",
    "        return x\n",
    "    a = dask.delayed(dask_fib(x-1))\n",
    "    b = dask.delayed(dask_fib(x-2))\n",
    "    c, d = dask.compute(a, b) # Compute in parallel\n",
    "    return c + d\n",
    "\n",
    "def seq_fib(x):\n",
    "    if x < 2:\n",
    "        return x\n",
    "    return seq_fib(x-1) + seq_fib(x-2)\n",
    "\n",
    "import functools\n",
    "@functools.lru_cache\n",
    "def fib(x):\n",
    "    if x < 2:\n",
    "        return x\n",
    "    return fib(x-1) + fib(x-2)\n",
    "\n",
    "import timeit\n",
    "seq_time = timeit.timeit(lambda: seq_fib(14), number=1)\n",
    "dask_time = timeit.timeit(lambda: dask_fib(14), number=1)\n",
    "memoized_time = timeit.timeit(lambda: fib(14), number=1)\n",
    "print(\"In sequence {}, in parallel {}, memoized\".format(seq_time, dask_time, memoized_time))\n",
    "#end::fib_task_hello_world[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a14077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::fail_to_ser[]\n",
    "class ConnectionClass:\n",
    "    def __init__(self, host, port):\n",
    "        import socket\n",
    "        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.socket.connect((host, port))\n",
    "\n",
    "@dask.delayed\n",
    "def bad_fun(x):\n",
    "    return ConnectionClass(\"www.scalingpythonml.com\", 80)\n",
    "\n",
    "# Fails to serialize\n",
    "if False:\n",
    "    dask.compute(bad_fun(1))\n",
    "#end::fail_to_ser[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21146d4-55e7-4f79-8e5a-69564a11b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From ch2 for visualize\n",
    "@dask.delayed\n",
    "def crawl(url, depth=0, maxdepth=1, maxlinks=4):\n",
    "    links = []\n",
    "    link_futures = []\n",
    "    try:\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        f = requests.get(url)\n",
    "        links += [(url, f.text)]\n",
    "        if (depth > maxdepth):\n",
    "            return links # base case\n",
    "        soup = BeautifulSoup(f.text, 'html.parser')\n",
    "        c = 0\n",
    "        for link in soup.find_all('a'):\n",
    "            if \"href\" in link:\n",
    "                c = c + 1\n",
    "                link_futures += crawl(link[\"href\"], depth=(depth+1), maxdepth=maxdepth)\n",
    "                # Don't branch too much were still in local mode and the web is big\n",
    "                if c > maxlinks:\n",
    "                    break\n",
    "        for r in dask.compute(link_futures):\n",
    "            links += r\n",
    "        return links\n",
    "    except requests.exceptions.InvalidSchema:\n",
    "        return [] # Skip non-web links\n",
    "import dask.bag as db\n",
    "githubs = [\"https://github.com/scalingpythonml/scalingpythonml\", \"https://github.com/dask/distributed\"]\n",
    "initial_bag = db.from_delayed(map(crawl, githubs))\n",
    "words_bag = initial_bag.map(lambda url_contents: url_contents[1].split(\" \")).flatten()\n",
    "#tag::visualize[]\n",
    "dask.visualize(words_bag.frequencies())\n",
    "#end::visualize[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dedab56-bfba-4477-99f6-9b166f5470d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::custom_serializer_not_own_class[]\n",
    "\n",
    "class SerConnectionClass:\n",
    "    def __init__(self, conn):\n",
    "        import socket\n",
    "        self.conn = conn\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state_dict = {\"host\": self.conn.socket.getpeername()[0], \"port\": self.conn.socket.getpeername()[1]}\n",
    "        return state_dict\n",
    "\n",
    "    def __setsate__(self, state):\n",
    "        self.conn = ConnectionClass(state[\"host\"], state[\"port\"])\n",
    "#end::custom_serializer_not_own_class[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df27b0-5bac-4354-9812-ebb4a4693e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can sort of serialize the connection\n",
    "@dask.delayed\n",
    "def ok_fun(x):\n",
    "    return SerConnectionClass(ConnectionClass(\"www.scalingpythonml.com\", 80))\n",
    "\n",
    "dask.compute(ok_fun(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de225f-f615-44b4-9b47-656a6ffa62bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://github.com/dask/distributed/issues/5561\n",
    "@dask.delayed\n",
    "def bad_fun(x):\n",
    "    return ConnectionClass(\"www.scalingpythonml.com\", 80)\n",
    "\n",
    "from distributed.protocol import dask_serialize, dask_deserialize\n",
    "\n",
    "@dask_serialize.register(ConnectionClass)\n",
    "def serialize(bad: ConnectionClass) -> Tuple[Dict, List[bytes]]:\n",
    "    import cloudpickle\n",
    "    header = {}\n",
    "    frames = [cloudpickle.dumps({\"host\": bad.socket.getpeername()[0], \"port\": bad.socket.getpeername()[1]})]\n",
    "    return header, frames\n",
    "\n",
    "@dask_deserialize.register(ConnectionClass)\n",
    "def deserialize(bad: Dict, frames: List[bytes]) -> ConnectionClass:\n",
    "    import cloudpickle\n",
    "    info = cloudpickle.loads(frames[0])\n",
    "    return ConnectionClass(info[\"host\"], info[\"port\"])\n",
    "\n",
    "# note: this does not work because dask_serialize didn't make it to the worker :/\n",
    "# dask.compute(bad_fun(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064d281-7e92-4ae7-8789-1799643d913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::serialize_class_with_numpy[]\n",
    "class NumpyInfo:\n",
    "    def __init__(self, name: str, features: npt.ArrayLike):\n",
    "        self.name = name\n",
    "        self.features = features\n",
    "        \n",
    "i = NumpyInfo(\"boo\", np.array(0))\n",
    "numpybits = [i]\n",
    "\n",
    "# Surprisingly this works, despite the implication that we would need to call register_generic\n",
    "from distributed.protocol import register_generic\n",
    "register_generic(NumpyInfo)\n",
    "\n",
    "dask.compute(ok_fun(1))\n",
    "#end::serialize_class_with_numpy[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72574a86-a627-4a9e-97c3-8edeaa7560b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(ok_fun(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda76d9b-88d8-456d-834f-e6274efcaa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_fun(1).visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3fb444-9804-4f34-86cf-78b1bd232522",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_fun(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351e5526-3ac3-4b36-b8be-d433919fd90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "#tag::make_chunked_array[]\n",
    "distributed_array = da.from_array(list(range(0, 10000)), chunks=10)\n",
    "#end::make_chunked_array[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602a26f-8877-4c10-ad2e-0f7b8fbd1683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From ch2 so we can continue the WC example\n",
    "@dask.delayed\n",
    "def crawl(url, depth=0, maxdepth=1, maxlinks=4):\n",
    "    links = []\n",
    "    link_futures = []\n",
    "    try:\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        f = requests.get(url)\n",
    "        links += [(url, f.text)]\n",
    "        if (depth > maxdepth):\n",
    "            return links # base case\n",
    "        soup = BeautifulSoup(f.text, 'html.parser')\n",
    "        c = 0\n",
    "        for link in soup.find_all('a'):\n",
    "            if \"href\" in link:\n",
    "                c = c + 1\n",
    "                link_futures += crawl(link[\"href\"], depth=(depth+1), maxdepth=maxdepth)\n",
    "                # Don't branch too much were still in local mode and the web is big\n",
    "                if c > maxlinks:\n",
    "                    break\n",
    "        for r in dask.compute(link_futures):\n",
    "            links += r\n",
    "        return links\n",
    "    except requests.exceptions.InvalidSchema:\n",
    "        return [] # Skip non-web links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ba92d-8544-45b5-9a45-4fdf5cc2e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "githubs = [\"https://github.com/scalingpythonml/scalingpythonml\", \"https://github.com/dask/distributed\"]\n",
    "some_bag = db.from_delayed(map(crawl, githubs))\n",
    "#tag::repartition_bag[]\n",
    "some_bag.repartition(npartitions=10)\n",
    "#end::repartition_bag[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02227f6-c277-40a4-8794-2dd5f408d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_bag.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5ca89b-5a08-4d49-9b1d-3e692d9d4d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_array.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cccb96-ac1b-425c-b09a-c56b902397be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.from_dask_array(distributed_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac90a7b1-85cb-4cc3-88a9-9db358144ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce62d96-c94d-4b59-a3d1-77cf5ac7f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::manual_persist[]\n",
    "df.persist\n",
    "# You do a bunch of things on DF\n",
    "\n",
    "# I'm done!\n",
    "from distributed.client import futures_of\n",
    "list(map(lambda x: x.release(), futures_of(df)))\n",
    "#end::manual_persist[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befdd364-eda4-458e-a4ef-1f8c69a0bc28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
