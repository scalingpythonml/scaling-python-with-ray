{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7610fd-b6c4-4984-b236-559852e53ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import dask\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import dask.bag as bag\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9308a-5ae6-4cb6-af5c-cf65be3dff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: if we were on a cluster we'd have to do more magic to install it on all the nodes in the cluster.\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d6808-2ecd-43de-8815-9ce4ac507d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8458fe54-c9b7-4df5-8199-74104571c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e39a8-81ae-4726-af9e-04310eef5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::custom_load[]\n",
    "def discover_files(path: str):\n",
    "    (fs, fspath) = fsspec.core.url_to_fs(path)\n",
    "    return (fs, fs.expand_path(fspath, recursive=\"true\"))\n",
    "def load_file(fs, file):\n",
    "    \"\"\"Load (and initially process) the data.\"\"\"\n",
    "    from PyPDF2 import PdfReader\n",
    "    try:\n",
    "        file_contents = fs.open(file)\n",
    "        pdf = PdfReader(file_contents)\n",
    "        return (file, pdf.pages[0].extract_text())\n",
    "    except Exception as e:\n",
    "        return (file, e)\n",
    "def load_data(path: str):\n",
    "    (fs, files) = discover_files(path)\n",
    "    bag_filenames = bag.from_sequence(files)\n",
    "    contents = bag_filenames.map(lambda f:load_file(fs, f))\n",
    "    return contents\n",
    "#end::custom_load[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a53a5-bd90-4626-9051-400d42e8dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::preprocess_json[]\n",
    "def make_url(idx):\n",
    "    page_size = 100\n",
    "    start = idx * page_size\n",
    "    return f\"https://api.fda.gov/food/enforcement.json?limit={page_size}&skip={start}\"\n",
    "urls = list(map(make_url, range(0, 10)))\n",
    "# Since they are multi-line json we can't use the default \\n line delim\n",
    "raw_json = bag.read_text(urls, linedelimiter=\"NODELIM\")\n",
    "def clean_records(raw_records):\n",
    "    import json\n",
    "    # We don't need the meta field just the results field\n",
    "    return json.loads(raw_records)[\"results\"]\n",
    "cleaned_records = raw_json.map(clean_records).flatten()\n",
    "# And now we can convert it to a DataFrame\n",
    "df = bag.Bag.to_dataframe(cleaned_records)\n",
    "#end::preprocess_json[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78625e9-9cfe-483d-bf5c-3e93ea052a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a27963-c6a9-4976-8e56-6fc50cd01fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files = discover_files(\"file:///tmp/a0\")\n",
    "list(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188380ec-52b7-4545-a00a-e948a2936eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(\"file:///tmp/pdfs\").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b383b5-d3a6-40a9-b2b2-ee423315f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::parallel_list[]\n",
    "def parallel_recursive_list(path: str, fs = None) -> List[str]:\n",
    "    print(f\"Listing {path}\")\n",
    "    if fs is None:\n",
    "        (fs, path) = fsspec.core.url_to_fs(path)\n",
    "    info = []\n",
    "    infos = fs.ls(path, detail=True)\n",
    "    # Above could throw PermissionError, but if we can't list the dir it's probably wrong so let it bubble up\n",
    "    files = []\n",
    "    dirs = []\n",
    "    for i in infos:\n",
    "        if i[\"type\"] == \"directory\":\n",
    "            # You can speed this up by using futures, covered in \"Advanced Scheduling\"\n",
    "            dir_list = dask.delayed(parallel_recursive_list)(i[\"name\"], fs=fs)\n",
    "            dirs += dir_list\n",
    "        else:\n",
    "            files.append(i[\"name\"])\n",
    "    for sub_files in dask.compute(dirs):\n",
    "        files.extend(sub_files)\n",
    "    return files\n",
    "#end::parallel_list[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17b8b6-8018-4b2f-9e8c-8c0c447542e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files = parallel_recursive_list(\"file:///tmp/a0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e2b75-b71e-417b-af8a-095a923c24d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e90f265-dc7b-4f1e-ab41-12a3fc7899bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::parallel_list_large[]\n",
    "def parallel_list_directories_recursive(path: str, fs = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively find all the sub directories.\n",
    "    \"\"\"\n",
    "    if fs is None:\n",
    "        (fs, path) = fsspec.core.url_to_fs(path)\n",
    "    info = []\n",
    "    # Ideally we could filter for directories here, but fsspec lacks that (for now)\n",
    "    infos = fs.ls(path, detail=True)\n",
    "    # Above could throw PermissionError, but if we can't list the dir it's probably wrong so let it bubble up\n",
    "    dirs = []\n",
    "    result = []\n",
    "    for i in infos:\n",
    "        if i[\"type\"] == \"directory\":\n",
    "            # You can speed this up by using futures, covered in \"Advanced Scheduling\"\n",
    "            result.append(i[\"name\"])\n",
    "            dir_list = dask.delayed(parallel_list_directories_recursive)(i[\"name\"], fs=fs)\n",
    "            dirs += dir_list\n",
    "    for sub_dirs in dask.compute(dirs):\n",
    "        result.extend(sub_dirs)\n",
    "    return result\n",
    "\n",
    "def list_files(path: str, fs = None) -> List[str]:\n",
    "    \"\"\"List files at a given depth with no recursion.\"\"\"\n",
    "    if fs is None:\n",
    "        (fs, path) = fsspec.core.url_to_fs(path)\n",
    "    info = []\n",
    "    # Ideally we could filter for directories here, but fsspec lacks that (for now)\n",
    "    return map(lambda i: i[\"name\"], filter(lambda i: i[\"type\"] == \"directory\", fs.ls(path, detail=True)))\n",
    "    \n",
    "\n",
    "def parallel_list_large(path: str, npartitions = None, fs = None) -> bag:\n",
    "    \"\"\"\n",
    "    Find all of the files (potentially too large to fit on the head node).\n",
    "    \"\"\"\n",
    "    directories = parallel_list_directories_recursive(path, fs = fs)\n",
    "    dir_bag = dask.bag.from_sequence(directories, npartitions = npartitions)\n",
    "    return dir_bag.map(lambda dir: list_files(dir, fs = fs)).flatten()\n",
    "#end::parallel_list_large[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd14a3b-68f0-4bd2-a267-3870173a3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_list_large(\"/tmp/a0\").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09bbb42-5dd2-404d-8f29-7c5a307ccf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::custom_load_nonfs[]\n",
    "def special_load_function(x):\n",
    "    ## Do your special loading logic in this function, like reading a database\n",
    "    return [\"Timbit\", \"Is\", \"Awesome\"][0: x % 4]\n",
    "\n",
    "partitions = bag.from_sequence(range(20), npartitions=5)\n",
    "raw_data = partitions.map(special_load_function).flatten()\n",
    "#end::custom_load_nonfs[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba521fb-c92a-43fd-845d-b4513b5b0dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372cfb0-8410-4305-aafb-89b1eba03812",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag.from_sequence(range(0,1000)).map(lambda x: (x, x)).foldby(lambda x, y: x + y, lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc376e1e-027d-480e-9c93-baa5d1a73e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fda754-f9da-4d9a-b029-94dcfb1eceea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
