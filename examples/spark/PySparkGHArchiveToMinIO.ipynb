{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::sparkImports[]\n",
    "from pyspark import *\n",
    "from pyspark.context import *\n",
    "from pyspark.conf import *\n",
    "#end::sparkImports[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::makeSparkConf[]\n",
    "conf = (SparkConf().setMaster(\"k8s://https://kubernetes.default\")\n",
    "#end::markMakeSparkConf[]\n",
    "    .set(\"spark.executor.instances\", \"2\")\n",
    "#tag::configureContainer[]\n",
    "    .set(\"spark.kubernetes.container.image\", \"holdenk/spark-py:v3.0.1.2\")\n",
    "#end::configureContainer[]\n",
    "#tag::configureService[]\n",
    "    .set(\"spark.driver.port\", \"2222\") # Needs to match svc\n",
    "    .set(\"spark.driver.blockManager.port\", \"7777\")\n",
    "    .set(\"spark.driver.host\", \"driver-service.jhub.svc.cluster.local\") # Needs to match svc\n",
    "    .set(\"spark.driver.bindAddress\", \"0.0.0.0\") #  Otherwise tries to bind to svc IP, will fail\n",
    "#end::configureService[]\n",
    "    .set(\"spark.kubernetes.namespace\", \"spark\")\n",
    "    .set(\"spark.app.name\", \"PySparkHelloWorldInsideTheCluster\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::makeSparkWithConf[]\n",
    "sc = SparkContext(conf=conf)\n",
    "#end::makeSparkWithConf[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date=datetime.datetime(2020,10,1, 1)\n",
    "gh_archive_files = []\n",
    "while current_date < datetime.datetime.now() -  datetime.timedelta(days=1):\n",
    "    current_date = current_date + datetime.timedelta(hours=1)\n",
    "    datestring = f'{current_date.year}-{current_date.month:02}-{current_date.day:02}-{current_date.hour}'\n",
    "    gh_url = f'http://data.githubarchive.org/{datestring}.json.gz'\n",
    "    gh_archive_files.append(gh_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::sparkSQLImports[]\n",
    "from pyspark.sql import *\n",
    "#end::sparkSQLImports[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::makeSession[]\n",
    "# Make a session using the existing SparkContext or default config\n",
    "session = SparkSession.builder.getOrCreate()\n",
    "#end::makeSession[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::fetchGzippedURL[]\n",
    "def fetch_gzipped_file(url):\n",
    "    \"\"\"Fetch a gzipped file\"\"\"\n",
    "    import tempfile\n",
    "    import urllib.request\n",
    "    \n",
    "    tf = tempfile.NamedTemporaryFile()\n",
    "\n",
    "    # The default urlretrive user agent seems to be on the do not allow list\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    urllib.request.install_opener(opener)\n",
    "    urllib.request.urlretrieve(url, tf.name)\n",
    "    \n",
    "    # Load data and decompress, I _think_ this streams data\n",
    "    import gzip\n",
    "    fh = gzip.open(tf.name, 'r')\n",
    "    while True:\n",
    "        line = fh.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        yield line\n",
    "#end::fetchGzippedURL[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification\n",
    "r = fetch_gzipped_file('https://data.githubarchive.org/2020-10-01-2.json.gz')\n",
    "list(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_archive_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://data.githubarchive.org/2020-10-01-2.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::loadData[]\n",
    "# Unlike in Dask we can't directly load from HTTP so we'll do some magic\n",
    "rdd_of_urls = sc.parallelize([gh_archive_files[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_of_records = rdd_of_urls.flatMap(fetch_gzipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_of_records.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_of_records.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session.read.json(rdd_of_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end::loadData[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
