{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f882c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::make_dask_client[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c72491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f8a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client() # Here we could specify a cluster, defaults to local mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2767582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#end::make_dask_client[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0805ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::sleepy_task_hello_world[]\n",
    "import timeit\n",
    "\n",
    "def slow_task(x):\n",
    "    import time\n",
    "    time.sleep(2) # Do something sciency/business\n",
    "    return x\n",
    "\n",
    "things = range(10)\n",
    "\n",
    "very_slow_result = map(slow_task, things)\n",
    "slowish_result = map(dask.delayed(slow_task), things)\n",
    "\n",
    "slow_time = timeit.timeit(lambda: list(very_slow_result), number=1)\n",
    "fast_time = timeit.timeit(lambda: list(dask.compute(*slowish_result)), number=1)\n",
    "print(\"In sequence {}, in parallel {}\".format(slow_time, fast_time))\n",
    "#end::sleepy_task_hello_world[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea537b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: if we were on a cluster we'd have to do more magislowish_result = map(dask.delayed(slow_task), things)c to install it on all the nodes in the cluster.\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::mini_crawl_task[]\n",
    "@dask.delayed\n",
    "def crawl(url, depth=0, maxdepth=1, maxlinks=4):\n",
    "    links = []\n",
    "    link_futures = []\n",
    "    try:\n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        f = requests.get(url)\n",
    "        links += [(url, f.text)]\n",
    "        if (depth > maxdepth):\n",
    "            return links # base case\n",
    "        soup = BeautifulSoup(f.text, 'html.parser')\n",
    "        c = 0\n",
    "        for link in soup.find_all('a'):\n",
    "            if \"href\" in link:\n",
    "                c = c + 1\n",
    "                link_futures += crawl(link[\"href\"], depth=(depth+1), maxdepth=maxdepth)\n",
    "                # Don't branch too much were still in local mode and the web is big\n",
    "                if c > maxlinks:\n",
    "                    break\n",
    "        for r in dask.compute(link_futures):\n",
    "            links += r\n",
    "        return links\n",
    "    except requests.exceptions.InvalidSchema:\n",
    "        return [] # Skip non-web links\n",
    "\n",
    "dask.compute(crawl(\"http://holdenkarau.com/\"))\n",
    "#end::mini_crawl_task[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::make_bag_of_crawler[]\n",
    "import dask.bag as db\n",
    "githubs = [\"https://github.com/scalingpythonml/scalingpythonml\", \"https://github.com/dask/distributed\"]\n",
    "initial_bag = db.from_delayed(map(crawl, githubs))\n",
    "#end::make_bag_of_crawler[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::make_a_bag_of_words[]\n",
    "words_bag = initial_bag.map(lambda url_contents: url_contents[1].split(\" \")).flatten()\n",
    "#end::make_a_bag_of_words[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ae294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::wc_freq[]\n",
    "dask.compute(words_bag.frequencies())\n",
    "#end::wc_freq[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99380f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::wc_func[]\n",
    "def make_word_tuple(w):\n",
    "    return (w, 1)\n",
    "\n",
    "def get_word(word_count):\n",
    "    return word_count[0]\n",
    "\n",
    "def sum_word_counts(wc1, wc2):\n",
    "    return (wc1[0], wc1[1] + wc2[1])\n",
    "\n",
    "word_count = words_bag.map(make_word_tuple).foldby(get_word, sum_word_counts)\n",
    "#end::wc_func[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc206c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.compute(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27117b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag::wc_dataframe\n",
    "import dask.dataframe as dd\n",
    "\n",
    "@dask.delayed\n",
    "def crawl_to_df(url, depth=0, maxdepth=1, maxlinks=4):\n",
    "    import pandas as pd\n",
    "    crawled = crawl(url, depth=depth, maxdepth=maxdepth, maxlinks=maxlinks)\n",
    "    return pd.DataFrame(crawled.compute(), columns=[\"url\", \"text\"]).set_index(\"url\")\n",
    "\n",
    "delayed_dfs = map(crawl_to_df, githubs)\n",
    "initial_df = dd.from_delayed(delayed_dfs)\n",
    "wc_df = initial_df.text.str.split().explode().value_counts()\n",
    "\n",
    "dask.compute(wc_df)\n",
    "#end::wc_dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
